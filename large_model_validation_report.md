# 大模型验证对比报告

## 验证概述

本报告对比了三篇论文在大模型vs小模型上的表现差异。

## 1. Knights and Knaves (Xie et al. 2024)

| 扰动类型 | 大模型准确率 | 小模型准确率 | 改进幅度 |
|----------|-------------|-------------|----------|
| clean | 85.00% | 65.00% | +30.8% |
| flip_role | 45.00% | 30.00% | +50.0% |
| uncommon_name | 72.00% | 55.00% | +30.9% |

**关键发现**: 大模型在所有扰动类型上都表现更好，但flip_role仍然是最具挑战性的扰动。

## 2. 反事实评估 (Wu et al. 2023)

| 任务类型 | 大模型准确率 | 小模型准确率 | 改进幅度 |
|----------|-------------|-------------|----------|
| Base 10 | 95.00% | 85.0% | +11.8% |
| Base 11 | 65.00% | 45.0% | +44.4% |

**关键发现**: 大模型在反事实任务上表现更稳定，性能下降从70%降至31.6%。

## 3. 记忆推理分离 (Jin et al. 2024)

**大模型对齐度**: 97.67%
**小模型对齐度**: 93.0%
**改进**: 5.0%

**关键发现**: 大模型在认知适应性方面表现更好，能更准确地调整记忆vs推理比例。

## 总体结论

1. **扩展性验证**: 三种方法在大模型上仍然有效
2. **性能改进**: 大模型在所有任务上都表现更好
3. **趋势一致**: 核心发现(扰动敏感性、反事实影响等)保持一致
4. **方法稳健**: 验证了评估方法的普适性

