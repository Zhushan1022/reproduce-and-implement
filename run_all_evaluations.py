#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–vsæ¨ç†èƒ½åŠ›è¯„ä¼° - ç»Ÿä¸€å¯åŠ¨è„šæœ¬

æ•´åˆæ‰€æœ‰å¤ç°çš„è®ºæ–‡æ–¹æ³•ï¼Œä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°
"""

import sys
import os
import time
import json
from typing import Dict, List, Optional
import argparse

# å¯¼å…¥æœ¬åœ°LLMæ¥å£
from local_llm_interface import load_model, SUPPORTED_MODELS

# å¯¼å…¥å„è¯„ä¼°æ–¹æ³•
sys.path.append('3/counterfactual-evaluation-master/')
from local_model_eval import run_comprehensive_evaluation as run_counterfactual_eval

sys.path.append('1/mem-kk-logic-main/')
from local_model_eval_kk import run_kk_evaluation

sys.path.append('4/Disentangling-Memory-and-Reasoning-main/')
from local_model_eval_jin import run_jin_evaluation

def test_model_availability(model_key: str) -> bool:
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    print(f"ğŸ” æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§: {model_key}")
    
    try:
        llm = load_model(model_key)
        
        # ç®€å•æµ‹è¯•
        test_prompt = "What is 2+2?"
        response = llm.query(test_prompt, max_new_tokens=50)
        
        llm.cleanup()
        
        if response and len(response.strip()) > 0:
            print(f"âœ… æ¨¡å‹ {model_key} å¯ç”¨")
            return True
        else:
            print(f"âŒ æ¨¡å‹ {model_key} æ— å›åº”")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_key} ä¸å¯ç”¨: {e}")
        return False

def run_single_evaluation(method: str, model_key: str) -> Optional[Dict]:
    """è¿è¡Œå•ä¸ªè¯„ä¼°æ–¹æ³•"""
    print(f"\n{'='*20} {method.upper()} è¯„ä¼° {'='*20}")
    
    try:
        if method == "counterfactual":
            # Wu et al. (2023) åäº‹å®è¯„ä¼°
            print("ğŸ“Š è¿è¡Œåäº‹å®è¯„ä¼° (Wu et al. 2023)")
            from local_model_eval import compare_bases
            return compare_bases(model_key, num_problems=20)
            
        elif method == "knights_knaves":
            # Xie et al. (2024) Knights and Knaves
            print("ğŸ§© è¿è¡ŒKnights and Knavesè¯„ä¼° (Xie et al. 2024)")
            from local_model_eval_kk import LocalKKEvaluator
            evaluator = LocalKKEvaluator(model_key)
            result = evaluator.compare_perturbations(limit=10)
            evaluator.cleanup_model()
            return result
            
        elif method == "memory_reasoning":
            # Jin et al. (2024) è®°å¿†æ¨ç†åˆ†ç¦»
            print("ğŸ”¬ è¿è¡Œè®°å¿†æ¨ç†åˆ†ç¦»è¯„ä¼° (Jin et al. 2024)")
            return run_jin_evaluation(model_key)
            
        else:
            print(f"âŒ æœªçŸ¥è¯„ä¼°æ–¹æ³•: {method}")
            return None
            
    except Exception as e:
        print(f"âŒ {method} è¯„ä¼°å¤±è´¥: {e}")
        return {"error": str(e)}

def run_comprehensive_evaluation(model_keys: List[str], 
                               methods: List[str],
                               output_dir: str = "evaluation_results") -> Dict:
    """è¿è¡Œå…¨é¢è¯„ä¼°"""
    print("ğŸš€ å¯åŠ¨å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–vsæ¨ç†èƒ½åŠ›å…¨é¢è¯„ä¼°")
    print("="*70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ€»ä½“ç»“æœ
    all_results = {
        "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "models_tested": model_keys,
        "methods_used": methods,
        "results": {}
    }
    
    start_time = time.time()
    
    for model_key in model_keys:
        print(f"\nğŸ¯ è¯„ä¼°æ¨¡å‹: {model_key}")
        print("-" * 50)
        
        # æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§
        if not test_model_availability(model_key):
            all_results["results"][model_key] = {
                "status": "unavailable",
                "error": "Model not available"
            }
            continue
        
        # è¿è¡Œå„ä¸ªè¯„ä¼°æ–¹æ³•
        model_results = {}
        
        for method in methods:
            print(f"\nğŸ“‹ è¿è¡Œ {method} è¯„ä¼°...")
            
            method_start = time.time()
            result = run_single_evaluation(method, model_key)
            method_time = time.time() - method_start
            
            if result:
                result["evaluation_time"] = method_time
                model_results[method] = result
                print(f"âœ… {method} è¯„ä¼°å®Œæˆ ({method_time:.1f}ç§’)")
            else:
                model_results[method] = {
                    "error": "Evaluation failed",
                    "evaluation_time": method_time
                }
                print(f"âŒ {method} è¯„ä¼°å¤±è´¥")
        
        all_results["results"][model_key] = model_results
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        model_file = os.path.join(output_dir, f"{model_key}_results.json")
        with open(model_file, 'w', encoding='utf-8') as f:
            json.dump(model_results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ {model_key} ç»“æœå·²ä¿å­˜åˆ°: {model_file}")
    
    total_time = time.time() - start_time
    all_results["total_evaluation_time"] = total_time
    
    # ä¿å­˜æ€»ä½“ç»“æœ
    summary_file = os.path.join(output_dir, "comprehensive_results.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    generate_analysis_report(all_results, output_dir)
    
    print(f"\n{'='*70}")
    print("ğŸ‰ å…¨é¢è¯„ä¼°å®Œæˆï¼")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}/")
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {output_dir}/analysis_report.md")
    
    return all_results

def generate_analysis_report(results: Dict, output_dir: str):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    report_path = os.path.join(output_dir, "analysis_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–vsæ¨ç†èƒ½åŠ›è¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"**è¯„ä¼°æ—¶é—´**: {results['evaluation_time']}\n\n")
        f.write(f"**æ€»ç”¨æ—¶**: {results.get('total_evaluation_time', 0):.1f}ç§’\n\n")
        
        f.write("## è¯„ä¼°æ¦‚è§ˆ\n\n")
        f.write(f"- **æµ‹è¯•æ¨¡å‹**: {', '.join(results['models_tested'])}\n")
        f.write(f"- **è¯„ä¼°æ–¹æ³•**: {', '.join(results['methods_used'])}\n\n")
        
        f.write("## è¯¦ç»†ç»“æœ\n\n")
        
        for model_key, model_results in results["results"].items():
            f.write(f"### {model_key}\n\n")
            
            if "status" in model_results and model_results["status"] == "unavailable":
                f.write("âŒ **æ¨¡å‹ä¸å¯ç”¨**\n\n")
                continue
            
            for method, method_result in model_results.items():
                f.write(f"#### {method}\n\n")
                
                if "error" in method_result:
                    f.write(f"âŒ **å¤±è´¥**: {method_result['error']}\n\n")
                    continue
                
                # æ ¹æ®æ–¹æ³•ç±»å‹ç”Ÿæˆä¸åŒçš„æŠ¥å‘Š
                if method == "counterfactual":
                    if "base10" in method_result and "base11" in method_result:
                        base10_acc = method_result["base10"].get("accuracy", 0)
                        base11_acc = method_result["base11"].get("accuracy", 0)
                        f.write(f"- **Base 10 å‡†ç¡®ç‡**: {base10_acc:.2%}\n")
                        f.write(f"- **Base 11 å‡†ç¡®ç‡**: {base11_acc:.2%}\n")
                        if base10_acc > 0:
                            drop = (base10_acc - base11_acc) / base10_acc
                            f.write(f"- **æ€§èƒ½ä¸‹é™**: {drop:.2%}\n")
                
                elif method == "knights_knaves":
                    # åˆ†ææ‰°åŠ¨ç»“æœ
                    for perturb_type, perturb_result in method_result.items():
                        if isinstance(perturb_result, dict) and "accuracy" in perturb_result:
                            acc = perturb_result["accuracy"]
                            f.write(f"- **{perturb_type}**: {acc:.2%}\n")
                
                elif method == "memory_reasoning":
                    if "summary_analysis" in method_result:
                        summary = method_result["summary_analysis"]
                        f.write(f"- **æ•´ä½“è®°å¿†æ¯”ä¾‹**: {summary.get('overall_memory_ratio', 0):.2%}\n")
                        f.write(f"- **æ•´ä½“å¯¹é½åº¦**: {summary.get('overall_alignment', 0):.2%}\n")
                
                f.write("\n")
        
        f.write("## ç»“è®º\n\n")
        f.write("æ ¹æ®è¯„ä¼°ç»“æœï¼Œå¯ä»¥å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š\n\n")
        f.write("1. **åäº‹å®è¯„ä¼°**ï¼šæ£€æµ‹æ¨¡å‹åœ¨åˆ†å¸ƒå¤–ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™\n")
        f.write("2. **é€»è¾‘æ‰°åŠ¨è¯„ä¼°**ï¼šåˆ†ææ¨¡å‹å¯¹ä¸åŒç±»å‹æ‰°åŠ¨çš„æ•æ„Ÿæ€§\n")
        f.write("3. **è®°å¿†æ¨ç†åˆ†ç¦»**ï¼šé‡åŒ–æ¨¡å‹ä¾èµ–è®°å¿†vsæ¨ç†çš„ç¨‹åº¦\n\n")
        f.write("è¯¦ç»†çš„æ•°å€¼ç»“æœè¯·å‚è€ƒå¯¹åº”çš„JSONæ–‡ä»¶ã€‚\n")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–vsæ¨ç†èƒ½åŠ›è¯„ä¼°")
    
    parser.add_argument("--models", nargs="+", 
                       default=["qwen-0.5b"],
                       choices=list(SUPPORTED_MODELS.keys()),
                       help="è¦æµ‹è¯•çš„æ¨¡å‹")
    
    parser.add_argument("--methods", nargs="+",
                       default=["counterfactual", "knights_knaves", "memory_reasoning"],
                       choices=["counterfactual", "knights_knaves", "memory_reasoning"],
                       help="è¦è¿è¡Œçš„è¯„ä¼°æ–¹æ³•")
    
    parser.add_argument("--output-dir", default="evaluation_results",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    
    parser.add_argument("--test-only", action="store_true",
                       help="ä»…æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§")
    
    args = parser.parse_args()
    
    print("ğŸ”¬ å¤§è¯­è¨€æ¨¡å‹è®°å¿†åŒ–vsæ¨ç†èƒ½åŠ›è¯„ä¼°å·¥å…·")
    print("="*50)
    
    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print("ğŸ“‹ æ”¯æŒçš„æ¨¡å‹:")
    for key, config in SUPPORTED_MODELS.items():
        print(f"  {key}: {config['model_name']}")
    print()
    
    if args.test_only:
        # ä»…æµ‹è¯•æ¨¡å‹
        print("ğŸ” æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§...")
        for model_key in args.models:
            test_model_availability(model_key)
    else:
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        try:
            results = run_comprehensive_evaluation(
                model_keys=args.models,
                methods=args.methods,
                output_dir=args.output_dir
            )
        except KeyboardInterrupt:
            print("\nâŒ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    main() 